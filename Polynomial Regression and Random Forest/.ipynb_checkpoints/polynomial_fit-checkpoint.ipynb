{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "This file contains regression methodes: lineair regression on timeseries and polynomial regression on climate features vs production values. If you have no experience with Jupyter Notebook or Python, part of the results can be found [here](https://readmees.github.io/polynomial_fit.html).\n",
    "\n",
    "# Import and preprocess data\n",
    "To run this file you can press on 'Cell' and then on 'Run Cells'. You'll be asked multiple times to give input, this way you'll be able use the data in multiple ways. If you have more data in the future, you can add them in the same directory as the one with the Automonous Greenhouse Challenge teams. You can add new data by making a new directory. This directory needs to contain five csv files: Greenhouse_climate.csv, CropManagement.csv, Irrigation.csv, Production.csv and vip.csv. To run this specific code it's usefull to have the same features as the ones of the greenhouse challenge (see the data/DataReadMe.pdf file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize as minimize\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Import helper functions from polynomial_fit.py.\n",
    "from polynomial_fit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please add the directory you're data is in. \n",
      "         If you're using our original provided files this should be 'data'\n",
      "data\n"
     ]
    }
   ],
   "source": [
    "# Loads in all the data in the data repository, except for the readme.\n",
    "FileNotFound = True\n",
    "while FileNotFound:\n",
    "    try:\n",
    "        directory = input(\"\"\"Please add the directory you're data is in. \n",
    "         If you're using our original provided files this should be 'data'\\n\"\"\")\n",
    "        datasets = [filename for filename in os.listdir(directory)]\n",
    "        FileNotFound = False\n",
    "    except FileNotFoundError:\n",
    "        print(\"Please add a valid directory\")\n",
    "        FileNotFound = True\n",
    "try:\n",
    "    datasets.remove('DataReadMe.pdf')\n",
    "except ValueError:\n",
    "    print('Warning: the DataReadMe.pdf file is removed, please visit https://github.com/readmees/erudite2020.git\\\n",
    "         to download it again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates Python dictionaries to be able to access all the different datasets.\n",
    "data = {}\n",
    "for dataset in datasets:\n",
    "    GHClim = pd.read_csv(f\"data/{dataset}/Greenhouse_climate.csv\") \n",
    "    Crop = pd.read_csv(f\"data/{dataset}/CropManagement.csv\") \n",
    "    irri = pd.read_csv(f\"data/{dataset}/Irrigation.csv\") \n",
    "    prod = pd.read_csv(f\"data/{dataset}/Production.csv\") \n",
    "    vip = pd.read_csv(f\"data/{dataset}/vip.csv\") \n",
    "    GHClim = GHClim.drop(['VentLee', 'Ventwind', 'AssimLight', 'BlackScr', 'EnScr'], axis=1)\n",
    "    \n",
    "    # Fills all the NaN values except the ones in the first row.\n",
    "    GHClim.fillna(method='ffill', inplace=True)\n",
    "    Crop.fillna(method='ffill', inplace=True)\n",
    "    irri.fillna(method='ffill', inplace=True)\n",
    "    prod.fillna(method='ffill', inplace=True)\n",
    "    vip.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Fills all the possible skipped NaN values.\n",
    "    GHClim.fillna(method='bfill', inplace=True)\n",
    "    Crop.fillna(method='bfill', inplace=True)\n",
    "    irri.fillna(method='bfill', inplace=True)\n",
    "    prod.fillna(method='bfill', inplace=True)\n",
    "    vip.fillna(method='bfill', inplace=True)\n",
    "    data[dataset] = {'GHClim':GHClim, 'Crop':Crop, 'irri':irri, 'prod':prod, 'vip':vip}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature information is retrieved from the ReadMe.pdf from \n",
    "# the Automonous greenhouse challenge.\n",
    "features_ghclim = {'Tair': ['Air temperature greenhouse', '°C'],\n",
    "'RHair': ['Relative humidity greenhouse', '%'],\n",
    "'CO2air': ['CO2 greenhouse', 'ppm'],\n",
    "'HumDef': ['Humidity deficit', 'g/m3'],\n",
    "'PipeLow': ['Lower circuit Temperature', '°C'],\n",
    "'PipeGrow': ['Growth circuit Temperature', '°C']}\n",
    "\n",
    "features_vip = {'CO2_Vip': ['CO2 setpoint', 'ppm'],\n",
    "'HumDef_Vip': ['humidity deficit setpoint', 'g/kg'],\n",
    "'MinPipeLow_Vip': ['net pipe minimum temperature setpoint', '°C'],\n",
    "'MinPipeGrow_Vip': ['crop pipe minimum temperature setpoint', '°C']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization\n",
    "Lets show how the features change with time, see the data/DataReadMe.pdf for more information about those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all a demonstration of the visualization of a single team/dataset, by default the Croperators. Since a lot of datapoint are the same for the teams, it's not always clear if there's only one datapoint measured or multiple. If you look closer, the points where multiple datapoints are measured are bigger points. This is not that userfriendly, which is why the regression line is added (which of course takes all datapoints into account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizes all the CO2air values over time for the Croperatures team,\n",
    "# and checks if the Croperatures is still part of the dataset\n",
    "key = 'CO2air'\n",
    "run = 'Maybe'\n",
    "while run != 'Yes' and run != 'No':\n",
    "    print('Please type Yes or No (case sensitive)')    \n",
    "    run = str(input('''Is the Croperators dataset still part of your datasets?.\n",
    "            [Yes/No]'''))\n",
    "if run == 'Yes':\n",
    "    visualize_time(data, ['Croperators'], key, features_ghclim[key][0], f'{key} in {features_ghclim[key][1]}')\n",
    "else:\n",
    "    while run != 'Yes':\n",
    "        try:\n",
    "            dataset = input('Which dataset would you like to show?\\n')\n",
    "            visualize_time(data, [dataset], key, features_ghclim[key][0], f'{key} in {features_ghclim[key][1]}')\n",
    "            run = 'Yes'\n",
    "        except KeyError:\n",
    "            print('Please chose a valid dataset (case sensitive)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greenhouse climate\n",
    "The next visualisations will be of the GHClim dataset.\n",
    "\n",
    "*Column heading Parameter description Unit Interval Dataset name Type Comments Data collection*\n",
    "![](https://imgur.com/iAl45aq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries.\n",
    "for key in features_ghclim:\n",
    "    visualize_time(data, datasets, key, features_ghclim[key][0], f'{key} in {features_ghclim[key][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vip\n",
    "The next visualisations will be of the Vip dataset\n",
    "\n",
    "*Column heading Parameter description Unit Interval Dataset name Type Comments Data collection*\n",
    "![](https://i.imgur.com/gm6EiUg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot timeseries.\n",
    "for key in features_vip:\n",
    "    visualize_time(data, datasets, key, features_vip[key][0], f'{key} in {features_vip[key][1]}', 'vip', 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature GHtime is the timestamp per five minutes. Every time it changes with +- 0.0034722, this is $\\frac{1}{1440}$ of a day: five minutes. So the GHtime is in days. One week is zeven days, so if we would like to take the average of every week, we would need to take the average of time the GHtime, changed by 7. 1 week is 10080 minutes. So 1 week is $\\frac{10080}{5}$ timestamps, so 2016 timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the user would like to devide the data in weekly data.\n",
    "run = 'Maybe'\n",
    "while run != 'Yes' and run != 'No':\n",
    "    print('Please type Yes or No (case sensitive)')    \n",
    "    run = str(input('''\n",
    "            Would you like to divide the data in weekly data? \n",
    "            If you choose No, the average of the features per dataset\n",
    "            will be calculated.\\n[Yes/No]'''))\n",
    "\n",
    "# Devide the data in weekly data\n",
    "if run == 'Yes':\n",
    "    weekly_data = calculate_weekly_data(data, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial fitting\n",
    "## On greenhouse climate data versus production A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_polynome(x,y, degree, x_name, y_name):\n",
    "    plt.plot(x, y, '.g')\n",
    "    params = np.polyfit(x,y, degree)\n",
    "    \n",
    "    # The more sample on the linspace, the more accurate the local maxima will be.\n",
    "    x_polynome = np.linspace(min(x),max(x),1000)\n",
    "    y_polynome = np.poly1d(params)(linspace)\n",
    "    plt.plot(x_polynome, y_polynome)\n",
    "    \n",
    "    # Find the maximum values.\n",
    "    peaks, _ = find_peaks(y_polynome, height=0)\n",
    "    plt.plot(x_polynome[peaks], y_polynome[peaks], 'xr')\n",
    "    plt.xlabel(f'{x_name} in {features_ghclim[x_name][1]}')\n",
    "    plt.ylabel(y_name)\n",
    "    plt.title(f'Polynomial fit on {features_ghclim[x_name][0]} vs {y_name}')\n",
    "    plt.show()\n",
    "    print(f'The optimal value(s) according to a polynomial\\nwith a degree of {degree}:', end=\" \")\n",
    "    peaks_str = \"\"\n",
    "    for val in x_polynome[peaks]:\n",
    "        peaks_str += f'{val}, '\n",
    "    print(peaks_str[:-2]+'.')\n",
    "\n",
    "# Ask for input.\n",
    "degree = 0\n",
    "while not 2<=degree<=20:\n",
    "    try:\n",
    "        degree = int(input('Which degree of polynomial fitting would you like?\\n'))\n",
    "        if not 2<=degree<=20:\n",
    "            print('Please chose an integer (a high degree polynomial may overfit)')\n",
    "    except ValueError:\n",
    "        print('Please chose an integer (a high degree polynomial may overfit)')\n",
    "if run == 'Yes':\n",
    "    for y_name in ['prodA', 'prodB']:\n",
    "        for x_name in ['CO2air', 'HumDef', 'Tair']:\n",
    "            x, y = weekly_data[x_name], weekly_data[y_name]\n",
    "            plot_polynome(x,y, degree, x_name, y_name)\n",
    "else:\n",
    "    # Take the average of the ghclim features and the eventual production value of every team.\n",
    "    plot_data = dict()\n",
    "    for category in ['ProdA_cum', 'ProdB_cum']:\n",
    "        for key in features_ghclim:\n",
    "            feature_point, prod = [], []\n",
    "            for dataset in datasets:\n",
    "                feature_point.append(data[dataset]['GHClim'][key].mean())\n",
    "                prod_df = data[dataset]['prod'][category]\n",
    "                prod.append(prod_df[prod_df.index[-1]])\n",
    "            x, y = feature_point, prod\n",
    "            plot_polynome(x,y, degree, key, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "*More results can be found in the [README.md](https://github.com/readmees/erudite2020/blob/master/README.md) and on the webpage mentioned in the start of this file: [readmees.github.io/polynomial_fit.html](https://readmees.github.io/polynomial_fit.html).*\n",
    "\n",
    "As you can see in the following figure, according to Zheng et al. (2018) the optimal CO growth for three perennial grass species is around 915 ppm. If you fit an 3rd degree polynomial on the automonous greenhouse challenge data you will get a simulair result, +- 947.475264901824 ppm (see figure 2)\n",
    "![](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12870-018-1243-3/MediaObjects/12870_2018_1243_Fig1_HTML.gif?as=webp)\n",
    "Figure 1: Zheng, Y., Li, F., Hao, L., Shedayi, A. A., Guo, L., Ma, C., ... & Xu, M. (2018). The optimal CO 2 concentrations for the growth of three perennial grass species. BMC plant biology, 18(1), 27.\n",
    "![](https://i.imgur.com/eA9OZZi.png)\n",
    "Figure 2: Automonous greenhouse challenge data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
